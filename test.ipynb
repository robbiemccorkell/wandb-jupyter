{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update imports when files change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates a (relatively) minimal example of using Ray Tune and Pytorch Lightning to train a fully connected network with optimal hyperparameters on an iris dataset.\n",
    "\n",
    "The `hpo` method is provided to kick off an example training process, and repeated to help test wandb logging within a jupyter notebook.\n",
    "\n",
    "To demonstrate the different methods of logging in this environment, the `logger` argument is provided.\n",
    "\n",
    "- `logger=\"lightning` will use the `lightning.pytorch.loggers.WandbLogger` module to log the results within each HPO trial.\n",
    "- `logger=\"ray\"` will use the `ray.air.integrations.wandb.WandbLoggerCallback` module to log the results as part of the HPO Tuner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hpo import hpo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First run with `logger=\"lightning\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-12-11 19:18:29</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:52.18        </td></tr>\n",
       "<tr><td>Memory:      </td><td>13.7/18.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/12 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  train_accuracy</th><th style=\"text-align: right;\">   val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_tune_9546a_00000</td><td>TERMINATED</td><td>127.0.0.1:14930</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">    0.0680161  </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         44.2425</td><td style=\"text-align: right;\"> 0.0470041  </td><td style=\"text-align: right;\">        1       </td><td style=\"text-align: right;\">0.114881   </td></tr>\n",
       "<tr><td>train_tune_9546a_00001</td><td>TERMINATED</td><td>127.0.0.1:14927</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">    0.00268882 </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         27.4551</td><td style=\"text-align: right;\"> 6.05167e-05</td><td style=\"text-align: right;\">        1       </td><td style=\"text-align: right;\">0.0143013  </td></tr>\n",
       "<tr><td>train_tune_9546a_00002</td><td>TERMINATED</td><td>127.0.0.1:14924</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">    0.000519428</td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         27.6522</td><td style=\"text-align: right;\"> 0.00806381 </td><td style=\"text-align: right;\">        1       </td><td style=\"text-align: right;\">0.00319099 </td></tr>\n",
       "<tr><td>train_tune_9546a_00003</td><td>TERMINATED</td><td>127.0.0.1:14929</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">    0.024383   </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         34.0813</td><td style=\"text-align: right;\"> 6.03477e-06</td><td style=\"text-align: right;\">        1       </td><td style=\"text-align: right;\">8.73751e-05</td></tr>\n",
       "<tr><td>train_tune_9546a_00004</td><td>TERMINATED</td><td>127.0.0.1:14933</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">    0.00226016 </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         44.3061</td><td style=\"text-align: right;\"> 0.000136771</td><td style=\"text-align: right;\">        1       </td><td style=\"text-align: right;\">0.0766294  </td></tr>\n",
       "<tr><td>train_tune_9546a_00005</td><td>TERMINATED</td><td>127.0.0.1:14925</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">    0.0913462  </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         33.7082</td><td style=\"text-align: right;\"> 4.96705e-09</td><td style=\"text-align: right;\">        1       </td><td style=\"text-align: right;\">0.0141029  </td></tr>\n",
       "<tr><td>train_tune_9546a_00006</td><td>TERMINATED</td><td>127.0.0.1:14928</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">    0.00195429 </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         27.1909</td><td style=\"text-align: right;\"> 0.0354798  </td><td style=\"text-align: right;\">        0.982143</td><td style=\"text-align: right;\">0.00949431 </td></tr>\n",
       "<tr><td>train_tune_9546a_00007</td><td>TERMINATED</td><td>127.0.0.1:14926</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">    0.0227947  </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         33.9517</td><td style=\"text-align: right;\"> 7.47587e-05</td><td style=\"text-align: right;\">        1       </td><td style=\"text-align: right;\">0.142688   </td></tr>\n",
       "<tr><td>train_tune_9546a_00008</td><td>TERMINATED</td><td>127.0.0.1:14932</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">    0.0731784  </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         44.3466</td><td style=\"text-align: right;\"> 0.0684356  </td><td style=\"text-align: right;\">        1       </td><td style=\"text-align: right;\">0.0295348  </td></tr>\n",
       "<tr><td>train_tune_9546a_00009</td><td>TERMINATED</td><td>127.0.0.1:14931</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">    0.0181125  </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         44.1321</td><td style=\"text-align: right;\"> 0          </td><td style=\"text-align: right;\">        1       </td><td style=\"text-align: right;\">0.0250322  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_tune pid=14927)\u001b[0m GPU available: True (mps), used: True\n",
      "\u001b[36m(train_tune pid=14927)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_tune pid=14927)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_tune pid=14927)\u001b[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[36m(train_tune pid=14927)\u001b[0m wandb: Currently logged in as: robbie-leap (leap-labs). Use `wandb login --relogin` to force relogin\n",
      "\u001b[36m(train_tune pid=14924)\u001b[0m wandb: Tracking run with wandb version 0.18.7\n",
      "\u001b[36m(train_tune pid=14924)\u001b[0m wandb: Run data is saved locally in ./wandb/run-20241211_191743-ayb53whj\n",
      "\u001b[36m(train_tune pid=14924)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[36m(train_tune pid=14924)\u001b[0m wandb: Syncing run stellar-jazz-196\n",
      "\u001b[36m(train_tune pid=14924)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/leap-labs/hanging-runs-test\n",
      "\u001b[36m(train_tune pid=14924)\u001b[0m wandb: 🚀 View run at https://wandb.ai/leap-labs/hanging-runs-test/runs/ayb53whj\n",
      "\u001b[36m(train_tune pid=14929)\u001b[0m \n",
      "\u001b[36m(train_tune pid=14929)\u001b[0m   | Name    | Type             | Params | Mode \n",
      "\u001b[36m(train_tune pid=14929)\u001b[0m -----------------------------------------------------\n",
      "\u001b[36m(train_tune pid=14929)\u001b[0m 0 | model   | Sequential       | 4.7 K  | train\n",
      "\u001b[36m(train_tune pid=14929)\u001b[0m 1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "\u001b[36m(train_tune pid=14929)\u001b[0m -----------------------------------------------------\n",
      "\u001b[36m(train_tune pid=14929)\u001b[0m 4.7 K     Trainable params\n",
      "\u001b[36m(train_tune pid=14929)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_tune pid=14929)\u001b[0m 4.7 K     Total params\n",
      "\u001b[36m(train_tune pid=14929)\u001b[0m 0.019     Total estimated model params size (MB)\n",
      "\u001b[36m(train_tune pid=14929)\u001b[0m 7         Modules in train mode\n",
      "\u001b[36m(train_tune pid=14929)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_tune pid=14929)\u001b[0m /Users/robbiemccorkell/Desktop/wandb-jupyter/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(train_tune pid=14933)\u001b[0m \n",
      "\u001b[36m(train_tune pid=14933)\u001b[0m 0 | model   | Sequential       | 403    | train\n",
      "\u001b[36m(train_tune pid=14933)\u001b[0m 403       Trainable params\n",
      "\u001b[36m(train_tune pid=14933)\u001b[0m 403       Total params\n",
      "\u001b[36m(train_tune pid=14930)\u001b[0m \n",
      "\u001b[36m(train_tune pid=14924)\u001b[0m \n",
      "\u001b[36m(train_tune pid=14932)\u001b[0m \n",
      "\u001b[36m(train_tune pid=14926)\u001b[0m \n",
      "\u001b[36m(train_tune pid=14927)\u001b[0m \n",
      "\u001b[36m(train_tune pid=14925)\u001b[0m \n",
      "\u001b[36m(train_tune pid=14928)\u001b[0m \n",
      "\u001b[36m(train_tune pid=14931)\u001b[0m \n",
      "\u001b[36m(train_tune pid=14929)\u001b[0m /Users/robbiemccorkell/Desktop/wandb-jupyter/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(train_tune pid=14929)\u001b[0m /Users/robbiemccorkell/Desktop/wandb-jupyter/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_tune pid=14928)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-17-36/train_tune_9546a_00006_6_batch_size=64,hidden_dim=16,learning_rate=0.0020_2024-12-11_19-17-37/checkpoint_000000)\n",
      "\u001b[36m(train_tune pid=14928)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-17-36/train_tune_9546a_00006_6_batch_size=64,hidden_dim=16,learning_rate=0.0020_2024-12-11_19-17-37/checkpoint_000001)\n",
      "\u001b[36m(train_tune pid=14926)\u001b[0m GPU available: True (mps), used: True\u001b[32m [repeated 9x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(train_tune pid=14926)\u001b[0m TPU available: False, using: 0 TPU cores\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14926)\u001b[0m HPU available: False, using: 0 HPUs\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14931)\u001b[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14931)\u001b[0m wandb: Currently logged in as: robbie-leap (leap-labs). Use `wandb login --relogin` to force relogin\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14927)\u001b[0m wandb: Tracking run with wandb version 0.18.7\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14927)\u001b[0m wandb: Run data is saved locally in ./wandb/run-20241211_191743-6efrjjcz\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14927)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14927)\u001b[0m wandb: Syncing run peachy-spaceship-201\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14927)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/leap-labs/hanging-runs-test\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14927)\u001b[0m wandb: 🚀 View run at https://wandb.ai/leap-labs/hanging-runs-test/runs/6efrjjcz\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14931)\u001b[0m   | Name    | Type             | Params | Mode \u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14931)\u001b[0m -----------------------------------------------------\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14931)\u001b[0m 0 | model   | Sequential       | 4.7 K  | train\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14931)\u001b[0m 1 | loss_fn | CrossEntropyLoss | 0      | train\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14931)\u001b[0m 4.7 K     Trainable params\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14931)\u001b[0m 0         Non-trainable params\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14931)\u001b[0m 4.7 K     Total params\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14931)\u001b[0m 0.019     Total estimated model params size (MB)\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14931)\u001b[0m 7         Modules in train mode\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14931)\u001b[0m 0         Modules in eval mode\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14931)\u001b[0m /Users/robbiemccorkell/Desktop/wandb-jupyter/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14928)\u001b[0m 0 | model   | Sequential       | 403    | train\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14928)\u001b[0m 403       Trainable params\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14928)\u001b[0m 403       Total params\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14932)\u001b[0m /Users/robbiemccorkell/Desktop/wandb-jupyter/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14932)\u001b[0m /Users/robbiemccorkell/Desktop/wandb-jupyter/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14927)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-17-36/train_tune_9546a_00001_1_batch_size=64,hidden_dim=128,learning_rate=0.0027_2024-12-11_19-17-37/checkpoint_000070)\u001b[32m [repeated 488x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14927)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-17-36/train_tune_9546a_00001_1_batch_size=64,hidden_dim=128,learning_rate=0.0027_2024-12-11_19-17-37/checkpoint_000155)\u001b[32m [repeated 607x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14927)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-17-36/train_tune_9546a_00001_1_batch_size=64,hidden_dim=128,learning_rate=0.0027_2024-12-11_19-17-37/checkpoint_000252)\u001b[32m [repeated 708x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14927)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-17-36/train_tune_9546a_00001_1_batch_size=64,hidden_dim=128,learning_rate=0.0027_2024-12-11_19-17-37/checkpoint_000345)\u001b[32m [repeated 690x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14927)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-17-36/train_tune_9546a_00001_1_batch_size=64,hidden_dim=128,learning_rate=0.0027_2024-12-11_19-17-37/checkpoint_000445)\u001b[32m [repeated 728x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14928)\u001b[0m `Trainer.fit` stopped: `max_epochs=500` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_tune pid=14927)\u001b[0m \u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mpeachy-spaceship-201\u001b[0m at: \u001b[34mhttps://wandb.ai/leap-labs/hanging-runs-test/runs/6efrjjcz\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_tune pid=14930)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-17-36/train_tune_9546a_00000_0_batch_size=16,hidden_dim=128,learning_rate=0.0680_2024-12-11_19-17-37/checkpoint_000279)\u001b[32m [repeated 662x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14924)\u001b[0m `Trainer.fit` stopped: `max_epochs=500` reached.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14930)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-17-36/train_tune_9546a_00000_0_batch_size=16,hidden_dim=128,learning_rate=0.0680_2024-12-11_19-17-37/checkpoint_000350)\u001b[32m [repeated 510x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_tune pid=14926)\u001b[0m \u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mlilac-totem-201\u001b[0m at: \u001b[34mhttps://wandb.ai/leap-labs/hanging-runs-test/runs/vtams9ya\u001b[0m\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_tune pid=14929)\u001b[0m `Trainer.fit` stopped: `max_epochs=500` reached.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14930)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-17-36/train_tune_9546a_00000_0_batch_size=16,hidden_dim=128,learning_rate=0.0680_2024-12-11_19-17-37/checkpoint_000443)\u001b[32m [repeated 373x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=14931)\u001b[0m `Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "2024-12-11 19:18:29,803\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-17-36' in 0.0415s.\n",
      "2024-12-11 19:18:29,806\tINFO tune.py:1041 -- Total run time: 52.20 seconds (52.14 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'hidden_dim': 64, 'learning_rate': 0.024382960114596945, 'batch_size': 32}\n"
     ]
    }
   ],
   "source": [
    "hpo(num_samples=10, num_epochs=500, logger=\"lightning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second run with `logger=\"lightning\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-12-11 19:19:19</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:49.84        </td></tr>\n",
       "<tr><td>Memory:      </td><td>13.1/18.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/12 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  train_accuracy</th><th style=\"text-align: right;\">   val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_tune_b477e_00000</td><td>TERMINATED</td><td>127.0.0.1:15643</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">    0.000853058</td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         42.0809</td><td style=\"text-align: right;\"> 0.00274658 </td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">0.0304653  </td></tr>\n",
       "<tr><td>train_tune_b477e_00001</td><td>TERMINATED</td><td>127.0.0.1:15647</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">    0.0388364  </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         33.0497</td><td style=\"text-align: right;\"> 2.90565e-06</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">1.06325e-05</td></tr>\n",
       "<tr><td>train_tune_b477e_00002</td><td>TERMINATED</td><td>127.0.0.1:15646</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">    0.0119395  </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         26.3473</td><td style=\"text-align: right;\"> 0.000199908</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">0.151583   </td></tr>\n",
       "<tr><td>train_tune_b477e_00003</td><td>TERMINATED</td><td>127.0.0.1:15640</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">    0.00185389 </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         41.9073</td><td style=\"text-align: right;\"> 0.00798562 </td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">0.0153228  </td></tr>\n",
       "<tr><td>train_tune_b477e_00004</td><td>TERMINATED</td><td>127.0.0.1:15642</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">    0.0620237  </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         26.3267</td><td style=\"text-align: right;\"> 1.23071e-05</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">1.15235e-06</td></tr>\n",
       "<tr><td>train_tune_b477e_00005</td><td>TERMINATED</td><td>127.0.0.1:15644</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">    0.037667   </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         26.0372</td><td style=\"text-align: right;\"> 1.2986e-05 </td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">0.248536   </td></tr>\n",
       "<tr><td>train_tune_b477e_00006</td><td>TERMINATED</td><td>127.0.0.1:15645</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">    0.0777279  </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         32.5205</td><td style=\"text-align: right;\"> 3.12923e-07</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">4.06976e-05</td></tr>\n",
       "<tr><td>train_tune_b477e_00007</td><td>TERMINATED</td><td>127.0.0.1:15637</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">    0.00310119 </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         32.4846</td><td style=\"text-align: right;\"> 4.00185e-05</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">0.0367832  </td></tr>\n",
       "<tr><td>train_tune_b477e_00008</td><td>TERMINATED</td><td>127.0.0.1:15639</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">    0.000449586</td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         42.1623</td><td style=\"text-align: right;\"> 0.0161911  </td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">0.0170677  </td></tr>\n",
       "<tr><td>train_tune_b477e_00009</td><td>TERMINATED</td><td>127.0.0.1:15638</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">    0.0280822  </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         32.6407</td><td style=\"text-align: right;\"> 5.04631e-06</td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">0.745489   </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_tune pid=15642)\u001b[0m GPU available: True (mps), used: True\n",
      "\u001b[36m(train_tune pid=15642)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_tune pid=15642)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_tune pid=15637)\u001b[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[36m(train_tune pid=15642)\u001b[0m wandb: Currently logged in as: robbie-leap (leap-labs). Use `wandb login --relogin` to force relogin\n",
      "\u001b[36m(train_tune pid=15644)\u001b[0m wandb: Tracking run with wandb version 0.18.7\n",
      "\u001b[36m(train_tune pid=15644)\u001b[0m wandb: Run data is saved locally in ./wandb/run-20241211_191834-lnoh1djj\n",
      "\u001b[36m(train_tune pid=15644)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[36m(train_tune pid=15644)\u001b[0m wandb: Syncing run gallant-sun-205\n",
      "\u001b[36m(train_tune pid=15644)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/leap-labs/hanging-runs-test\n",
      "\u001b[36m(train_tune pid=15644)\u001b[0m wandb: 🚀 View run at https://wandb.ai/leap-labs/hanging-runs-test/runs/lnoh1djj\n",
      "\u001b[36m(train_tune pid=15645)\u001b[0m \n",
      "\u001b[36m(train_tune pid=15645)\u001b[0m   | Name    | Type             | Params | Mode \n",
      "\u001b[36m(train_tune pid=15645)\u001b[0m -----------------------------------------------------\n",
      "\u001b[36m(train_tune pid=15645)\u001b[0m 0 | model   | Sequential       | 4.7 K  | train\n",
      "\u001b[36m(train_tune pid=15645)\u001b[0m 1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "\u001b[36m(train_tune pid=15645)\u001b[0m -----------------------------------------------------\n",
      "\u001b[36m(train_tune pid=15645)\u001b[0m 4.7 K     Trainable params\n",
      "\u001b[36m(train_tune pid=15645)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_tune pid=15645)\u001b[0m 4.7 K     Total params\n",
      "\u001b[36m(train_tune pid=15645)\u001b[0m 0.019     Total estimated model params size (MB)\n",
      "\u001b[36m(train_tune pid=15645)\u001b[0m 7         Modules in train mode\n",
      "\u001b[36m(train_tune pid=15645)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_tune pid=15645)\u001b[0m /Users/robbiemccorkell/Desktop/wandb-jupyter/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(train_tune pid=15644)\u001b[0m \n",
      "\u001b[36m(train_tune pid=15647)\u001b[0m \n",
      "\u001b[36m(train_tune pid=15637)\u001b[0m \n",
      "\u001b[36m(train_tune pid=15640)\u001b[0m \n",
      "\u001b[36m(train_tune pid=15640)\u001b[0m 0 | model   | Sequential       | 403    | train\n",
      "\u001b[36m(train_tune pid=15640)\u001b[0m 403       Trainable params\n",
      "\u001b[36m(train_tune pid=15640)\u001b[0m 403       Total params\n",
      "\u001b[36m(train_tune pid=15646)\u001b[0m \n",
      "\u001b[36m(train_tune pid=15642)\u001b[0m \n",
      "\u001b[36m(train_tune pid=15644)\u001b[0m /Users/robbiemccorkell/Desktop/wandb-jupyter/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(train_tune pid=15644)\u001b[0m /Users/robbiemccorkell/Desktop/wandb-jupyter/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_tune pid=15643)\u001b[0m \n",
      "\u001b[36m(train_tune pid=15639)\u001b[0m \n",
      "\u001b[36m(train_tune pid=15638)\u001b[0m \n",
      "\u001b[36m(train_tune pid=15645)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-18-29/train_tune_b477e_00006_6_batch_size=32,hidden_dim=64,learning_rate=0.0777_2024-12-11_19-18-29/checkpoint_000000)\n",
      "\u001b[36m(train_tune pid=15638)\u001b[0m GPU available: True (mps), used: True\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15638)\u001b[0m TPU available: False, using: 0 TPU cores\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15638)\u001b[0m HPU available: False, using: 0 HPUs\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15646)\u001b[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15638)\u001b[0m wandb: Currently logged in as: robbie-leap (leap-labs). Use `wandb login --relogin` to force relogin\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15643)\u001b[0m wandb: Tracking run with wandb version 0.18.7\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15643)\u001b[0m wandb: Run data is saved locally in ./wandb/run-20241211_191834-qv2jb0p8\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15643)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15643)\u001b[0m wandb: Syncing run dauntless-frog-212\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15643)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/leap-labs/hanging-runs-test\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15643)\u001b[0m wandb: 🚀 View run at https://wandb.ai/leap-labs/hanging-runs-test/runs/qv2jb0p8\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15638)\u001b[0m   | Name    | Type             | Params | Mode \u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15638)\u001b[0m -----------------------------------------------------\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15643)\u001b[0m 0 | model   | Sequential       | 1.3 K  | train\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15638)\u001b[0m 1 | loss_fn | CrossEntropyLoss | 0      | train\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15643)\u001b[0m 1.3 K     Trainable params\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15638)\u001b[0m 0         Non-trainable params\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15643)\u001b[0m 1.3 K     Total params\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15638)\u001b[0m 0.002     Total estimated model params size (MB)\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15638)\u001b[0m 7         Modules in train mode\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15638)\u001b[0m 0         Modules in eval mode\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15638)\u001b[0m /Users/robbiemccorkell/Desktop/wandb-jupyter/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15638)\u001b[0m 0 | model   | Sequential       | 403    | train\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15638)\u001b[0m 403       Trainable params\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15638)\u001b[0m 403       Total params\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15639)\u001b[0m /Users/robbiemccorkell/Desktop/wandb-jupyter/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15639)\u001b[0m /Users/robbiemccorkell/Desktop/wandb-jupyter/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15642)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-18-29/train_tune_b477e_00004_4_batch_size=64,hidden_dim=128,learning_rate=0.0620_2024-12-11_19-18-29/checkpoint_000059)\u001b[32m [repeated 434x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15642)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-18-29/train_tune_b477e_00004_4_batch_size=64,hidden_dim=128,learning_rate=0.0620_2024-12-11_19-18-29/checkpoint_000139)\u001b[32m [repeated 599x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15642)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-18-29/train_tune_b477e_00004_4_batch_size=64,hidden_dim=128,learning_rate=0.0620_2024-12-11_19-18-29/checkpoint_000247)\u001b[32m [repeated 813x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15642)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-18-29/train_tune_b477e_00004_4_batch_size=64,hidden_dim=128,learning_rate=0.0620_2024-12-11_19-18-29/checkpoint_000346)\u001b[32m [repeated 744x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15642)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-18-29/train_tune_b477e_00004_4_batch_size=64,hidden_dim=128,learning_rate=0.0620_2024-12-11_19-18-29/checkpoint_000448)\u001b[32m [repeated 777x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15644)\u001b[0m `Trainer.fit` stopped: `max_epochs=500` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_tune pid=15642)\u001b[0m \u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33msilvery-sun-211\u001b[0m at: \u001b[34mhttps://wandb.ai/leap-labs/hanging-runs-test/runs/n4mfrbwc\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_tune pid=15637)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-18-29/train_tune_b477e_00007_7_batch_size=32,hidden_dim=128,learning_rate=0.0031_2024-12-11_19-18-29/checkpoint_000428)\u001b[32m [repeated 653x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15642)\u001b[0m `Trainer.fit` stopped: `max_epochs=500` reached.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_tune pid=15637)\u001b[0m \u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mdark-darkness-207\u001b[0m at: \u001b[34mhttps://wandb.ai/leap-labs/hanging-runs-test/runs/g2kv9old\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_tune pid=15640)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-18-29/train_tune_b477e_00003_3_batch_size=16,hidden_dim=16,learning_rate=0.0019_2024-12-11_19-18-29/checkpoint_000357)\u001b[32m [repeated 543x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15647)\u001b[0m `Trainer.fit` stopped: `max_epochs=500` reached.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15640)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-18-29/train_tune_b477e_00003_3_batch_size=16,hidden_dim=16,learning_rate=0.0019_2024-12-11_19-18-29/checkpoint_000455)\u001b[32m [repeated 294x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=15640)\u001b[0m `Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "\u001b[36m(train_tune pid=16606)\u001b[0m GPU available: True (mps), used: True\n",
      "\u001b[36m(train_tune pid=16606)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_tune pid=16606)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_tune pid=16602)\u001b[0m \n",
      "\u001b[36m(train_tune pid=16602)\u001b[0m   | Name    | Type             | Params | Mode \n",
      "\u001b[36m(train_tune pid=16602)\u001b[0m -----------------------------------------------------\n",
      "\u001b[36m(train_tune pid=16602)\u001b[0m 0 | model   | Sequential       | 403    | train\n",
      "\u001b[36m(train_tune pid=16602)\u001b[0m 1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "\u001b[36m(train_tune pid=16602)\u001b[0m -----------------------------------------------------\n",
      "\u001b[36m(train_tune pid=16602)\u001b[0m 403       Trainable params\n",
      "\u001b[36m(train_tune pid=16602)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_tune pid=16602)\u001b[0m 403       Total params\n",
      "\u001b[36m(train_tune pid=16602)\u001b[0m 0.002     Total estimated model params size (MB)\n",
      "\u001b[36m(train_tune pid=16602)\u001b[0m 7         Modules in train mode\n",
      "\u001b[36m(train_tune pid=16602)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_tune pid=16602)\u001b[0m /Users/robbiemccorkell/Desktop/wandb-jupyter/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(train_tune pid=16606)\u001b[0m \n",
      "\u001b[36m(train_tune pid=16606)\u001b[0m 0 | model   | Sequential       | 1.3 K  | train\n",
      "\u001b[36m(train_tune pid=16606)\u001b[0m 1.3 K     Trainable params\n",
      "\u001b[36m(train_tune pid=16606)\u001b[0m 1.3 K     Total params\n",
      "\u001b[36m(train_tune pid=16605)\u001b[0m \n",
      "\u001b[36m(train_tune pid=16600)\u001b[0m \n",
      "\u001b[36m(train_tune pid=16598)\u001b[0m \n",
      "\u001b[36m(train_tune pid=16604)\u001b[0m \n",
      "\u001b[36m(train_tune pid=16599)\u001b[0m \n",
      "\u001b[36m(train_tune pid=16603)\u001b[0m \n",
      "\u001b[36m(train_tune pid=16601)\u001b[0m \n",
      "\u001b[36m(train_tune pid=16597)\u001b[0m \n",
      "\u001b[36m(train_tune pid=16606)\u001b[0m /Users/robbiemccorkell/Desktop/wandb-jupyter/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(train_tune pid=16599)\u001b[0m /Users/robbiemccorkell/Desktop/wandb-jupyter/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(train_tune pid=16599)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-19-51/train_tune_e5244_00005_5_batch_size=32,hidden_dim=16,learning_rate=0.0592_2024-12-11_19-19-51/checkpoint_000000)\n",
      "\u001b[36m(_WandbLoggingActor pid=16674)\u001b[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[36m(_WandbLoggingActor pid=16674)\u001b[0m wandb: Currently logged in as: robbie-leap (leap-labs). Use `wandb login --relogin` to force relogin\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb: Tracking run with wandb version 0.18.7\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-12-11_19-17-36_352360_14852/artifacts/2024-12-11_19-19-51/train_tune_2024-12-11_19-19-51/driver_artifacts/train_tune_e5244_00007_7_batch_size=16,hidden_dim=16,learning_rate=0.0002_2024-12-11_19-19-51/wandb/run-20241211_191959-e5244_00007\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb: Syncing run train_tune_e5244_00007\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/leap-labs/hanging-runs-test\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb: 🚀 View run at https://wandb.ai/leap-labs/hanging-runs-test/runs/e5244_00007\n",
      "\u001b[36m(train_tune pid=16601)\u001b[0m GPU available: True (mps), used: True\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16601)\u001b[0m TPU available: False, using: 0 TPU cores\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16601)\u001b[0m HPU available: False, using: 0 HPUs\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16597)\u001b[0m   | Name    | Type             | Params | Mode \u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16597)\u001b[0m -----------------------------------------------------\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16599)\u001b[0m 0 | model   | Sequential       | 403    | train\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16597)\u001b[0m 1 | loss_fn | CrossEntropyLoss | 0      | train\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16599)\u001b[0m 403       Trainable params\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16597)\u001b[0m 0         Non-trainable params\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16599)\u001b[0m 403       Total params\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16597)\u001b[0m 0.005     Total estimated model params size (MB)\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16597)\u001b[0m 7         Modules in train mode\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16597)\u001b[0m 0         Modules in eval mode\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16601)\u001b[0m /Users/robbiemccorkell/Desktop/wandb-jupyter/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16597)\u001b[0m 0 | model   | Sequential       | 1.3 K  | train\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16597)\u001b[0m 1.3 K     Trainable params\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16597)\u001b[0m 1.3 K     Total params\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16601)\u001b[0m /Users/robbiemccorkell/Desktop/wandb-jupyter/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16601)\u001b[0m /Users/robbiemccorkell/Desktop/wandb-jupyter/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16600)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-19-51/train_tune_e5244_00003_3_batch_size=64,hidden_dim=32,learning_rate=0.0071_2024-12-11_19-19-51/checkpoint_000002)\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb: Currently logged in as: robbie-leap (leap-labs). Use `wandb login --relogin` to force relogin\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb: Tracking run with wandb version 0.18.7\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-12-11_19-17-36_352360_14852/artifacts/2024-12-11_19-19-51/train_tune_2024-12-11_19-19-51/driver_artifacts/train_tune_e5244_00003_3_batch_size=64,hidden_dim=32,learning_rate=0.0071_2024-12-11_19-19-51/wandb/run-20241211_191959-e5244_00003\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb: Syncing run train_tune_e5244_00003\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/leap-labs/hanging-runs-test\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb: 🚀 View run at https://wandb.ai/leap-labs/hanging-runs-test/runs/e5244_00003\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16600)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-19-51/train_tune_e5244_00003_3_batch_size=64,hidden_dim=32,learning_rate=0.0071_2024-12-11_19-19-51/checkpoint_000050)\u001b[32m [repeated 373x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16600)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-19-51/train_tune_e5244_00003_3_batch_size=64,hidden_dim=32,learning_rate=0.0071_2024-12-11_19-19-51/checkpoint_000145)\u001b[32m [repeated 758x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16600)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-19-51/train_tune_e5244_00003_3_batch_size=64,hidden_dim=32,learning_rate=0.0071_2024-12-11_19-19-51/checkpoint_000227)\u001b[32m [repeated 659x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16600)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-19-51/train_tune_e5244_00003_3_batch_size=64,hidden_dim=32,learning_rate=0.0071_2024-12-11_19-19-51/checkpoint_000315)\u001b[32m [repeated 692x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16603)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-19-51/train_tune_e5244_00006_6_batch_size=32,hidden_dim=64,learning_rate=0.0036_2024-12-11_19-19-51/checkpoint_000317)\u001b[32m [repeated 648x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16600)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-19-51/train_tune_e5244_00003_3_batch_size=64,hidden_dim=32,learning_rate=0.0071_2024-12-11_19-19-51/checkpoint_000461)\u001b[32m [repeated 490x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16598)\u001b[0m `Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:                                                                                \n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb: Run history:\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:                    epoch ▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇█\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:                     step ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇█\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇████\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:         time_this_iter_s ▂▂▁▂▂▂▂▂▂▂▂▂▂▁▁▃▂▃▂▂▂▂▃▁▂▄▁▂▁▂▂▃█▂▃▃▂▂▄▂\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:             time_total_s ▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:                timestamp ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:           train_accuracy ▁▂▃▁▂▄▄▄▃▆▆▆▇▆▇▆▅▆▆▇▇▆▇▇▇▆▆▆▇▇▇▇█▇▇▇▇▇▇█\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:               train_loss ██▇▆▆▆▅▅▄▄▅▄▃▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:       training_iteration ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:             val_accuracy ▁▂▂▂▂▂▂▃▃▄▄▄▄▄▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██████████\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:                 val_loss ███▇▇▆▆▅▅▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb: Run summary:\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:                    epoch 499\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb: iterations_since_restore 500\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:                     step 1000\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:       time_since_restore 25.60306\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:         time_this_iter_s 0.04086\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:             time_total_s 25.60306\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:                timestamp 1733944835\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:           train_accuracy 0.96429\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:               train_loss 0.12066\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:       training_iteration 500\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:             val_accuracy 1\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb:                 val_loss 0.11029\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb: 🚀 View run train_tune_e5244_00009 at: https://wandb.ai/leap-labs/hanging-runs-test/runs/e5244_00009\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/leap-labs/hanging-runs-test\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[36m(_WandbLoggingActor pid=16679)\u001b[0m wandb: Find logs at: ./wandb/run-20241211_191959-e5244_00009/logs\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb:                    epoch ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇███\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb:                     step ▁▁▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇█\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb:         time_this_iter_s ▇▆█▁▁▁▁▂▁▁▁▂▁▁▁▂▂▁▁▅▂▂▁▁▁▂▁▂▁▂▁▂▁▂▂▂▅▁▃▃\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▆▆▆▆▆▆▆▇██\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb:           train_accuracy ▅█▁▅▅█▅██▅██▅█▅█████████████████████████\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb:               train_loss █▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb:             val_accuracy ██▁█████████████████████████▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb:                 val_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[36m(train_tune pid=16603)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-19-51/train_tune_e5244_00006_6_batch_size=32,hidden_dim=64,learning_rate=0.0036_2024-12-11_19-19-51/checkpoint_000419)\u001b[32m [repeated 469x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16600)\u001b[0m `Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb: \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb: Run history:\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb:                timestamp ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb: Run summary:\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb:                    epoch 499\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb: iterations_since_restore 500\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb:                     step 1000\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb:       time_since_restore 25.89194\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb:         time_this_iter_s 0.076\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb:             time_total_s 25.89194\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb:                timestamp 1733944836\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb:           train_accuracy 1\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb:               train_loss 0.00059\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb:       training_iteration 500\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb:             val_accuracy 0.96667\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb:                 val_loss 0.04133\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb: 🚀 View run train_tune_e5244_00003 at: https://wandb.ai/leap-labs/hanging-runs-test/runs/e5244_00003\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/leap-labs/hanging-runs-test\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[36m(_WandbLoggingActor pid=16694)\u001b[0m wandb: Find logs at: ./wandb/run-20241211_191959-e5244_00003/logs\n",
      "\u001b[36m(train_tune pid=16597)\u001b[0m `Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "\u001b[36m(train_tune pid=16603)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-19-51/train_tune_e5244_00006_6_batch_size=32,hidden_dim=64,learning_rate=0.0036_2024-12-11_19-19-51/checkpoint_000498)\u001b[32m [repeated 579x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb: Run history:\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb:                    epoch ▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇███\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▆▆▆▇▇▇▇▇█████\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb:                     step ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇█████\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb:       time_since_restore ▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb:         time_this_iter_s █▄▂▁▂▁▁▂▂▁▁▁▁▁▁▂▂▂▂▁▂▁▁▂▁▁▂▁▁▂▂▃▂▂▄▃▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb:                timestamp ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb:           train_accuracy ▁███▇█▇█▇███████████████████████████████\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb:               train_loss █▂▁▂▃▁▃▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇█\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb:             val_accuracy ▁▆▆▆████▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb:                 val_loss █▃▃▂▁▂▁▂▃▂▃▅▅▅▆▅▆█▇▅▅▅▆▅▅▆▆▆▆▆▆▆▆▇▆▇▆▆▇▇\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb: Run summary:\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb:                    epoch 499\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb: iterations_since_restore 500\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb:                     step 2000\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb:       time_since_restore 33.07882\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb:         time_this_iter_s 0.05252\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb:             time_total_s 33.07882\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb:                timestamp 1733944843\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb:           train_accuracy 1\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb:               train_loss 0.0002\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb:       training_iteration 500\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb:             val_accuracy 0.96667\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb:                 val_loss 0.15275\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb: 🚀 View run train_tune_e5244_00002 at: https://wandb.ai/leap-labs/hanging-runs-test/runs/e5244_00002\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/leap-labs/hanging-runs-test\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[36m(_WandbLoggingActor pid=16702)\u001b[0m wandb: Find logs at: ./wandb/run-20241211_191959-e5244_00002/logs\n",
      "\u001b[36m(_WandbLoggingActor pid=16671)\u001b[0m wandb:                    epoch ▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇███\n",
      "\u001b[36m(_WandbLoggingActor pid=16671)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇█████\n",
      "\u001b[36m(_WandbLoggingActor pid=16671)\u001b[0m wandb:                     step ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████\n",
      "\u001b[36m(_WandbLoggingActor pid=16671)\u001b[0m wandb:       time_since_restore ▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇█████\n",
      "\u001b[36m(_WandbLoggingActor pid=16671)\u001b[0m wandb:         time_this_iter_s █▃▅▁▁▁▁▁▁▁▁▂▃▁▂▂▁▁▂▂▂▁▁▁▂▂▂▁▂▂▂▁▁▃▂▁▁▁▁▂\n",
      "\u001b[36m(_WandbLoggingActor pid=16671)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇█████\n",
      "\u001b[36m(_WandbLoggingActor pid=16671)\u001b[0m wandb:           train_accuracy ▁███▁█▁▁██▁██▁▁████▁██▁█████████████████\n",
      "\u001b[36m(_WandbLoggingActor pid=16671)\u001b[0m wandb:               train_loss ▇▁▂▁▂▃▁▃▁▃▂▂▂█▃▁██▂▄▅▁▁▁▄▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16671)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[36m(_WandbLoggingActor pid=16671)\u001b[0m wandb:             val_accuracy ▁███▇████▇▇█████▇████▇█▇▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇\n",
      "\u001b[36m(_WandbLoggingActor pid=16671)\u001b[0m wandb:                 val_loss ▂▃▂▁▁▂▄▂▂▁▁▂▂▃▂▂▁▂▂▂▁▁▃▅█▆▄▃▃▄▄▅▅▅▅▅▅▅▅▅\n",
      "\u001b[36m(_WandbLoggingActor pid=16669)\u001b[0m wandb:                    epoch ▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇██\n",
      "\u001b[36m(_WandbLoggingActor pid=16669)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[36m(_WandbLoggingActor pid=16669)\u001b[0m wandb:                     step ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇████\n",
      "\u001b[36m(_WandbLoggingActor pid=16669)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇█\n",
      "\u001b[36m(_WandbLoggingActor pid=16669)\u001b[0m wandb:         time_this_iter_s ▃▃█▄▂▁▂▁▁▂▂▄▃▃▁▂▂▂▁▂▂▁▂▂▃▂▁▂▂▂▅▂▁▁▁▁▂▂▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16669)\u001b[0m wandb:             time_total_s ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇█████\n",
      "\u001b[36m(_WandbLoggingActor pid=16669)\u001b[0m wandb:           train_accuracy ███▁▁▁██████████████████████████████████\n",
      "\u001b[36m(_WandbLoggingActor pid=16669)\u001b[0m wandb:               train_loss █▁▁▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16669)\u001b[0m wandb:       training_iteration ▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[36m(_WandbLoggingActor pid=16669)\u001b[0m wandb:             val_accuracy ▁███████▆█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆\n",
      "\u001b[36m(_WandbLoggingActor pid=16669)\u001b[0m wandb:                 val_loss █▆▂▁▁▁▁▂▁▂▃▂▂▂▂▃▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:                    epoch ▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇█\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:                     step ▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇███\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇█████\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:         time_this_iter_s █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:           train_accuracy ▁▆▃██▆████▆█████████████████████████████\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:               train_loss ▃▂▅██▁▁▄▆▄▃▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:       training_iteration ▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:             val_accuracy █▁██████████████████████████████████████\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:                 val_loss █▇▆▅▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16665)\u001b[0m wandb:                    epoch ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[36m(_WandbLoggingActor pid=16665)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[36m(_WandbLoggingActor pid=16665)\u001b[0m wandb:                     step ▁▁▁▁▁▂▂▂▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇███\n",
      "\u001b[36m(_WandbLoggingActor pid=16665)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇█████\n",
      "\u001b[36m(_WandbLoggingActor pid=16665)\u001b[0m wandb:         time_this_iter_s ▃▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁█▃▆▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16665)\u001b[0m wandb:             time_total_s ▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆███\n",
      "\u001b[36m(_WandbLoggingActor pid=16665)\u001b[0m wandb:           train_accuracy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16665)\u001b[0m wandb:               train_loss ▆█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16665)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[36m(_WandbLoggingActor pid=16665)\u001b[0m wandb:             val_accuracy ██▁▁████▁▁▁▁▁▁▁▁▁▁▁▁▁▁██████████████████\n",
      "\u001b[36m(_WandbLoggingActor pid=16665)\u001b[0m wandb:                 val_loss █▁▇▂▁▁▁▁▁▂▂▂▂▃▃▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb:                    epoch ▁▁▁▁▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇██\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb:                     step ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb:       time_since_restore ▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇█████\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb:         time_this_iter_s ▇▄▁▁▁▁▁▁▁▁▁▁▃▂▄▁▁▂▁▁▁▁▂▁▂▂▃▁▆▂▂▁█▂▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb:           train_accuracy ▁▃▅▄█▇▇▇▅▇▇▇▇█▇▇▇▇▇██▇▇█▇███████▇▇█▇████\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb:               train_loss █▅▆▆▄▃▂▂▂▂▂▂▁▂▁▁▂▂▁▁▁▁▁▂▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb:       training_iteration ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb:             val_accuracy ▁▃▄▄▅▆▆▆▇███████████████████████████████\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb:                 val_loss █▄▄▄▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb: \u001b[32m [repeated 24x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16604)\u001b[0m `Trainer.fit` stopped: `max_epochs=500` reached.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_tune pid=16602)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-19-51/train_tune_e5244_00007_7_batch_size=16,hidden_dim=16,learning_rate=0.0002_2024-12-11_19-19-51/checkpoint_000439)\u001b[32m [repeated 199x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb: Run history:\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb:                timestamp ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb: Run summary:\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb:                    epoch 499\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb: iterations_since_restore 500\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb:                     step 2000\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb:       time_since_restore 34.12026\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb:         time_this_iter_s 0.06192\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb:             time_total_s 34.12026\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb:                timestamp 1733944843\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb:           train_accuracy 1\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb:               train_loss 0.02849\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb:       training_iteration 500\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb:             val_accuracy 1\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb:                 val_loss 0.01991\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb: 🚀 View run train_tune_e5244_00004 at: https://wandb.ai/leap-labs/hanging-runs-test/runs/e5244_00004\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/leap-labs/hanging-runs-test\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16700)\u001b[0m wandb: Find logs at: ./wandb/run-20241211_191959-e5244_00004/logs\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:                                                                                \n",
      "\u001b[36m(_WandbLoggingActor pid=16674)\u001b[0m wandb:                    epoch ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[36m(_WandbLoggingActor pid=16674)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇███\n",
      "\u001b[36m(_WandbLoggingActor pid=16674)\u001b[0m wandb:                     step ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇██\n",
      "\u001b[36m(_WandbLoggingActor pid=16674)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████\n",
      "\u001b[36m(_WandbLoggingActor pid=16674)\u001b[0m wandb:         time_this_iter_s ▄▃▃▃▃▃█▃█▄▅▇▄▄▄▆▄▆▄▄▃▅▇▅▅▃▄▄▄▃▁▂▂▂▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16674)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████\n",
      "\u001b[36m(_WandbLoggingActor pid=16674)\u001b[0m wandb:           train_accuracy ▁▅▆█▅▆█▆█▆██▅▅▆█▆▆▆██▆█████████▆█▆██▅█▆█\n",
      "\u001b[36m(_WandbLoggingActor pid=16674)\u001b[0m wandb:               train_loss ██▇▇▆▅▅▅▄▃▃▃▃▂▂▃▂▂▂▂▂▁▂▂▂▁▂▂▁▁▂▁▂▁▁▁▃▂▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16674)\u001b[0m wandb:       training_iteration ▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[36m(_WandbLoggingActor pid=16674)\u001b[0m wandb:             val_accuracy ▁▆▆▆▆▆▇▇▇▇▇▇▇▇▇█████████████████████████\n",
      "\u001b[36m(_WandbLoggingActor pid=16674)\u001b[0m wandb:                 val_loss ███▆▆▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[36m(train_tune pid=16602)\u001b[0m `Trainer.fit` stopped: `max_epochs=500` reached.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb: Run history:\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:                    epoch ▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇█\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:                     step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇██\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:         time_this_iter_s █▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:                timestamp ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:           train_accuracy ▁▁▁▅█▅▅▁▁▅▅██▁▁█▅▅███▁▅█████▅███▅▅██████\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:               train_loss █▆▆▄▃▃▄▄▃▂▃▃▂▃▃▂▃▃▂▂▂▂▃▂▂▂▁▁▁▁▁▁▂▁▂▁▂▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:       training_iteration ▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇██\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:             val_accuracy ▁▁▂▂▂▂▅▇▇▇▅▅▅▆▇▇▇▇▇███▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:                 val_loss ██▇▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb: Run summary:\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:                    epoch 499\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb: iterations_since_restore 500\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:                     step 4000\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:       time_since_restore 44.51086\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:         time_this_iter_s 0.03965\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:             time_total_s 44.51086\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:                timestamp 1733944851\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:           train_accuracy 1\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:               train_loss 0.02486\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:       training_iteration 500\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:             val_accuracy 0.96667\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb:                 val_loss 0.06231\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb: 🚀 View run train_tune_e5244_00007 at: https://wandb.ai/leap-labs/hanging-runs-test/runs/e5244_00007\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb: ⭐️ View project at: https://wandb.ai/leap-labs/hanging-runs-test\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[36m(_WandbLoggingActor pid=16666)\u001b[0m wandb: Find logs at: ./wandb/run-20241211_191959-e5244_00007/logs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 19:19:19,789\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-18-29' in 0.0722s.\n",
      "2024-12-11 19:19:19,793\tINFO tune.py:1041 -- Total run time: 49.85 seconds (49.77 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'hidden_dim': 128, 'learning_rate': 0.06202366177593322, 'batch_size': 64}\n"
     ]
    }
   ],
   "source": [
    "hpo(num_samples=10, num_epochs=500, logger=\"lightning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run with `logger=\"ray\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-12-11 19:20:52</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:00.42        </td></tr>\n",
       "<tr><td>Memory:      </td><td>12.3/18.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/12 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  train_accuracy</th><th style=\"text-align: right;\">   val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_tune_e5244_00000</td><td>TERMINATED</td><td>127.0.0.1:16604</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">    0.0240247  </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         34.18  </td><td style=\"text-align: right;\"> 9.4374e-08 </td><td style=\"text-align: right;\">        1       </td><td style=\"text-align: right;\">0.0188939  </td></tr>\n",
       "<tr><td>train_tune_e5244_00001</td><td>TERMINATED</td><td>127.0.0.1:16605</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">    0.000218536</td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         43.79  </td><td style=\"text-align: right;\"> 0.0625065  </td><td style=\"text-align: right;\">        1       </td><td style=\"text-align: right;\">0.0544614  </td></tr>\n",
       "<tr><td>train_tune_e5244_00002</td><td>TERMINATED</td><td>127.0.0.1:16597</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">    0.00390696 </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         33.0788</td><td style=\"text-align: right;\"> 0.000197692</td><td style=\"text-align: right;\">        1       </td><td style=\"text-align: right;\">0.152746   </td></tr>\n",
       "<tr><td>train_tune_e5244_00003</td><td>TERMINATED</td><td>127.0.0.1:16600</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">    0.00707671 </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         25.8919</td><td style=\"text-align: right;\"> 0.000588747</td><td style=\"text-align: right;\">        1       </td><td style=\"text-align: right;\">0.0413282  </td></tr>\n",
       "<tr><td>train_tune_e5244_00004</td><td>TERMINATED</td><td>127.0.0.1:16601</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">    0.000107147</td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         34.1203</td><td style=\"text-align: right;\"> 0.0284889  </td><td style=\"text-align: right;\">        1       </td><td style=\"text-align: right;\">0.019909   </td></tr>\n",
       "<tr><td>train_tune_e5244_00005</td><td>TERMINATED</td><td>127.0.0.1:16599</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">    0.0592108  </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         33.503 </td><td style=\"text-align: right;\"> 0.000104843</td><td style=\"text-align: right;\">        1       </td><td style=\"text-align: right;\">0.0950233  </td></tr>\n",
       "<tr><td>train_tune_e5244_00006</td><td>TERMINATED</td><td>127.0.0.1:16603</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">    0.00364605 </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         33.5074</td><td style=\"text-align: right;\"> 1.40698e-05</td><td style=\"text-align: right;\">        1       </td><td style=\"text-align: right;\">3.85253e-05</td></tr>\n",
       "<tr><td>train_tune_e5244_00007</td><td>TERMINATED</td><td>127.0.0.1:16602</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">    0.000215408</td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         44.5109</td><td style=\"text-align: right;\"> 0.0248592  </td><td style=\"text-align: right;\">        1       </td><td style=\"text-align: right;\">0.0623124  </td></tr>\n",
       "<tr><td>train_tune_e5244_00008</td><td>TERMINATED</td><td>127.0.0.1:16606</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">    0.00854498 </td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         34.0636</td><td style=\"text-align: right;\"> 1.81295e-06</td><td style=\"text-align: right;\">        1       </td><td style=\"text-align: right;\">0.182212   </td></tr>\n",
       "<tr><td>train_tune_e5244_00009</td><td>TERMINATED</td><td>127.0.0.1:16598</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">    0.000192548</td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         25.6031</td><td style=\"text-align: right;\"> 0.120659   </td><td style=\"text-align: right;\">        0.964286</td><td style=\"text-align: right;\">0.110292   </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 19:19:51,613\tINFO wandb.py:319 -- Already logged into W&B.\n",
      "2024-12-11 19:19:59,115\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.039 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:19:59,123\tWARNING util.py:201 -- The `process_trial_result` operation took 1.048 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:19:59,124\tWARNING util.py:201 -- Processing trial results took 1.049 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2024-12-11 19:19:59,125\tWARNING util.py:201 -- The `process_trial_result` operation took 1.050 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:19:59,634\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.507 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:19:59,638\tWARNING util.py:201 -- The `process_trial_result` operation took 0.511 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:19:59,644\tWARNING util.py:201 -- Processing trial results took 0.518 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2024-12-11 19:19:59,645\tWARNING util.py:201 -- The `process_trial_result` operation took 0.519 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:20:00,309\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.663 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:20:00,311\tWARNING util.py:201 -- The `process_trial_result` operation took 0.664 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:20:00,312\tWARNING util.py:201 -- Processing trial results took 0.665 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2024-12-11 19:20:00,313\tWARNING util.py:201 -- The `process_trial_result` operation took 0.666 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:20:00,816\tWARNING util.py:201 -- The `process_trial_result` operation took 0.500 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:20:00,817\tWARNING util.py:201 -- Processing trial results took 0.501 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2024-12-11 19:20:00,817\tWARNING util.py:201 -- The `process_trial_result` operation took 0.501 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:20:02,222\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.512 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:20:02,223\tWARNING util.py:201 -- The `process_trial_result` operation took 0.513 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:20:02,224\tWARNING util.py:201 -- Processing trial results took 0.514 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2024-12-11 19:20:02,224\tWARNING util.py:201 -- The `process_trial_result` operation took 0.514 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:20:02,757\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.517 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:20:02,758\tWARNING util.py:201 -- The `process_trial_result` operation took 0.518 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:20:02,758\tWARNING util.py:201 -- Processing trial results took 0.518 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2024-12-11 19:20:02,759\tWARNING util.py:201 -- The `process_trial_result` operation took 0.518 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:20:03,265\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.505 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:20:03,266\tWARNING util.py:201 -- The `process_trial_result` operation took 0.506 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:20:03,267\tWARNING util.py:201 -- Processing trial results took 0.506 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2024-12-11 19:20:03,267\tWARNING util.py:201 -- The `process_trial_result` operation took 0.507 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:20:03,933\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.645 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:20:03,934\tWARNING util.py:201 -- The `process_trial_result` operation took 0.646 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:20:03,934\tWARNING util.py:201 -- Processing trial results took 0.646 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2024-12-11 19:20:03,934\tWARNING util.py:201 -- The `process_trial_result` operation took 0.646 s, which may be a performance bottleneck.\n",
      "2024-12-11 19:20:52,032\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/robbiemccorkell/ray_results/train_tune_2024-12-11_19-19-51' in 0.0937s.\n",
      "2024-12-11 19:20:53,685\tINFO tune.py:1041 -- Total run time: 62.08 seconds (60.32 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'hidden_dim': 64, 'learning_rate': 0.003646051886383178, 'batch_size': 32}\n"
     ]
    }
   ],
   "source": [
    "hpo(num_samples=10, num_epochs=500, logger=\"ray\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
